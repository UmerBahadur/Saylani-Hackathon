import json
import boto3
import pandas as pd
from io import StringIO

def lambda_handler(event, context):
    # Initialize AWS clients
    sqs = boto3.client('sqs')
    s3 = boto3.client('s3')
    sns = boto3.client('sns')

    # Your SQS queue URL
    queue_url = "https://sqs.ap-southeast-2.amazonaws.com/503561414499/CoinMarketCapSQS"
    
    # âœ… Fixed TopicArn (removed invalid ":uuid" at the end)
    topic_arn = "arn:aws:sns:ap-southeast-2:503561414499:CoinMarketCapTopic2"

    # Poll messages from SQS (max 10)
    response = sqs.receive_message(
        QueueUrl=queue_url,
        MaxNumberOfMessages=10,
        WaitTimeSeconds=0
    )

    if 'Messages' not in response:
        return {
            'statusCode': 200,
            'body': 'No messages in queue.'
        }

    messages = response['Messages']

    for msg in messages:
        try:
            # Parse SQS message body
            sqs_body = json.loads(msg['Body'])
            sns_message_str = sqs_body['Message']
            sns_message = json.loads(sns_message_str)

            # Debug prints to check message structure
            print("SQS Body:", msg['Body'])
            print("SNS Message (raw):", sns_message_str)
            print("SNS Message (parsed):", sns_message)

            # Only process if message contains expected S3 event structure
            if 'Records' in sns_message:
                s3_record = sns_message['Records'][0]['s3']
                bucket = s3_record['bucket']['name']
                key = s3_record['object']['key']

                # Fetch CSV content from S3
                obj = s3.get_object(Bucket=bucket, Key=key)
                csv_content = obj['Body'].read().decode('utf-8')

                # Load into pandas DataFrame
                df = pd.read_csv(StringIO(csv_content))

                # === Data processing step (customize as needed) ===
                df['processed_timestamp'] = pd.Timestamp.utcnow()

                # Convert processed DataFrame back to CSV
                csv_buffer = StringIO()
                df.to_csv(csv_buffer, index=False)

                # Define processed S3 path
                processed_bucket = bucket
                processed_key = key.replace('raw/', 'processed/')

                # Upload processed CSV to S3
                s3.put_object(
                    Bucket=processed_bucket,
                    Key=processed_key,
                    Body=csv_buffer.getvalue().encode('utf-8')
                )

                # ðŸ”” Publish SNS notification
                sns.publish(
                    TopicArn=topic_arn,
                    Subject='Processed Crypto Data',
                    Message=f"Processed file saved to s3://{processed_bucket}/{processed_key}"
                )
            else:
                print("Skipped message: No 'Records' key found in SNS message.")

            # âœ… Delete message from SQS whether or not it was processed
            sqs.delete_message(
                QueueUrl=queue_url,
                ReceiptHandle=msg['ReceiptHandle']
            )

        except Exception as e:
            print(f"Error processing message: {e}")
            # Optionally, you could send error alerts or move the message to a dead-letter queue (DLQ)

    return {
        'statusCode': 200,
        'body': f"Processed {len(messages)} messages."
    }
